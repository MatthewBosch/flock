microsoft/Phi-3.5-mini-instruct:
  per_device_train_batch_size: 2  # Increase batch size for better GPU utilization
  gradient_accumulation_steps: 1000  # Further reduce gradient accumulation steps to speed up training
  num_train_epochs: 2000
  lora_rank: 16  # Increase rank for richer parameter learning
  lora_alpha: 128  # Increase alpha for stronger updates
  lora_dropout: 0.05  # Lower dropout to allow more information retention
